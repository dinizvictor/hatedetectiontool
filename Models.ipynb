{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score #(TP+TN)/(TP+FP+FN+TN) >> Proporção de TP e TN sobre o total.\n",
    "from sklearn.metrics import f1_score #2*(pr*rec)/(pr+rec) >> Média harmônica entre Precision e Recall.\n",
    "from sklearn.metrics import roc_auc_score #Area Under the Receiver Operating Characteristic Curve.\n",
    "from sklearn.metrics import recall_score #(TP)/(TP+FN) >> Proporção de TP verdadeiramente classificados. (Gargalo)\n",
    "from sklearn.metrics import precision_score #(TP)/(TP+FP) >> Proporção de TP nas predições positivas.\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Addressing datasets\n",
    "        self.ds_fortuna = pd.read_csv(\"datasets/binary_classification.csv\")\n",
    "        self.ds_fortuna = self.ds_fortuna.drop(columns = [\"hatespeech_G1\",\"annotator_G1\",\"hatespeech_G2\",\n",
    "                                                        \"annotator_G2\",\"hatespeech_G3\",\"annotator_G3\"])\n",
    "        \n",
    "        self.ds_pelle = pd.read_csv(\"datasets/OffComBR2.txt\", sep=\",\", names = [\"label\",\"text\"])\n",
    "        \n",
    "        #Standardizing Paula Fortuna (2019) dataset\n",
    "        self.ds_fortuna = self.ds_fortuna.rename(columns = {\"hatespeech_comb\":\"label\"})\n",
    "        self.ds_fortuna[\"text\"] = self.ds_fortuna[\"text\"].replace(to_replace = \"@[A-Za-z0-9_]*\", value = \"\", regex = True) #Remover o @ dos usuários do tweet.\n",
    "        self.ds_fortuna[\"text\"] = self.ds_fortuna[\"text\"].replace(to_replace = \"\\\\n\", value = \" \", regex = True) #Remover o \\n do texto\n",
    "        self.ds_fortuna[\"text\"] = self.ds_fortuna[\"text\"].replace(to_replace = \"'\", value = \"\", regex = True) #Remover '' das palavras\n",
    "        self.ds_fortuna[\"text\"] = self.ds_fortuna[\"text\"].replace(to_replace = \"http[^ ]+\", value = \"\", regex = True) #Remover links\n",
    "        self.ds_fortuna[\"text\"] = self.ds_fortuna[\"text\"].replace(to_replace = \"www.[^ ]+\", value = \"\", regex = True) #Remover links\n",
    "        self.ds_fortuna[\"text\"] = self.ds_fortuna[\"text\"].str.lower() #Padronização de caixa\n",
    "        \n",
    "        #Standardizing Rogers de Pelle (2017) dataset\n",
    "        self.ds_pelle[\"text\"] = self.ds_pelle[\"text\"].replace(to_replace = \"'\", value = \"\", regex = True)\n",
    "        self.ds_pelle[\"label\"] = self.ds_pelle[\"label\"].replace(to_replace = \"yes\", value = 1, regex = True)\n",
    "        self.ds_pelle[\"label\"] = self.ds_pelle[\"label\"].replace(to_replace = \"no\", value = 0, regex = True)\n",
    "        self.ds_pelle = self.ds_pelle[[\"text\",\"label\"]]\n",
    "        self.ds_pelle[\"text\"] = self.ds_pelle[\"text\"].str.lower()\n",
    "        \n",
    "        #Experiment Outcomes\n",
    "        self.exp_counter = 1\n",
    "        self.bow_outcomes = pd.DataFrame(columns=[\"id\",\"tags\",\"feature\",\"ngram\",\"pelle\",\"split\",\n",
    "                                                  \"acc_lr\",\"rec_lr\",\"pr_lr\",\"f1_lr\",\"roc_auc_lr\",\n",
    "                                                  \"acc_nb\",\"rec_nb\",\"pr_nb\",\"f1_nb\",\"roc_auc_nb\",\n",
    "                                                  \"acc_svm\",\"rec_svm\",\"pr_svm\",\"f1_svm\",\"roc_auc_svm\",\n",
    "                                                  \"acc_rf\",\"rec_rf\",\"pr_rf\",\"f1_rf\",\"roc_auc_rf\",\n",
    "                                                  \"created\",\"elapsed\"])\n",
    "        \n",
    "        self.tfidf_outcomes = pd.DataFrame(columns=[\"id\",\"tags\",\"feature\",\"ngram\",\"pelle\",\"split\",\n",
    "                                                  \"acc_lr\",\"rec_lr\",\"pr_lr\",\"f1_lr\",\"roc_auc_lr\",\n",
    "                                                  \"acc_nb\",\"rec_nb\",\"pr_nb\",\"f1_nb\",\"roc_auc_nb\",\n",
    "                                                  \"acc_svm\",\"rec_svm\",\"pr_svm\",\"f1_svm\",\"roc_auc_svm\",\n",
    "                                                  \"acc_rf\",\"rec_rf\",\"pr_rf\",\"f1_rf\",\"roc_auc_rf\",\n",
    "                                                  \"created\",\"elapsed\"])\n",
    "        \n",
    "        self.w2v_outcomes = pd.DataFrame(columns=[\"id\",\"tags\",\"feature\",\"ngram\",\"pelle\",\"split\",\n",
    "                                                  \"acc_lr\",\"rec_lr\",\"pr_lr\",\"f1_lr\",\"roc_auc_lr\",\n",
    "                                                  \"acc_nb\",\"rec_nb\",\"pr_nb\",\"f1_nb\",\"roc_auc_nb\",\n",
    "                                                  \"acc_svm\",\"rec_svm\",\"pr_svm\",\"f1_svm\",\"roc_auc_svm\",\n",
    "                                                  \"acc_rf\",\"rec_rf\",\"pr_rf\",\"f1_rf\",\"roc_auc_rf\",\n",
    "                                                  \"created\",\"elapsed\"])\n",
    "        \n",
    "        self.rs_outcomes = pd.DataFrame(columns=[\"tags\",\"split\",\"n_sample\",\"feature\",\"ngram\",\"pelle\",\"f1_mean_test_score\",\n",
    "                                                 \"n_estimators\",\"max_features\",\"max_depth\",\"min_samples_split\",\n",
    "                                                 \"min_samples_leaf\",\"bootstrap\",\"class_weight\",\"created\"])\n",
    "        \n",
    "        #Preprocessing functions\n",
    "        def tknze(sentence):\n",
    "            space_tnkzr = tokenize.WhitespaceTokenizer()\n",
    "            return space_tnkzr.tokenize(sentence)\n",
    "\n",
    "        def joinTokens(wordlist):\n",
    "            sentence = \" \"\n",
    "            return sentence.join(wordlist) \n",
    "\n",
    "        def tweetTknze(sentence):\n",
    "            tt_tknzr = tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "            return joinTokens(tt_tknzr.tokenize(sentence))\n",
    "\n",
    "        def removeStopWords(sentence):\n",
    "            #https://virtuati.com.br/cliente/knowledgebase/25/Lista-de-StopWords.html\n",
    "            #https://gist.github.com/alopes/5358189\n",
    "            stopWords1 = ['a', 'agora', 'ainda', 'alguem', 'algum', 'alguma', 'algumas', 'alguns', 'ampla', 'amplas', 'amplo', 'amplos', 'ante', 'antes', 'ao', 'aos', 'apos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'ate', 'atraves', 'cada', 'coisa', 'coisas', 'com', 'como', 'contra', 'contudo', 'da', 'daquele', 'daqueles', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'deste', 'destes', 'deve', 'devem', 'devendo', 'dever', 'devera', 'deverao', 'deveria', 'deveriam', 'devia', 'deviam', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'do', 'dos', 'e', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'enquanto', 'entre', 'era', 'essa', 'essas', 'esse', 'esses', 'esta', 'esta', 'estamos', 'estao', 'estas', 'estava', 'estavam', 'estavamos', 'este', 'estes', 'estou', 'eu', 'fazendo', 'fazer', 'feita', 'feitas', 'feito', 'feitos', 'foi', 'for', 'foram', 'fosse', 'fossem', 'grande', 'grandes', 'ha', 'isso', 'isto', 'ja', 'la', 'la', 'lhe', 'lhes', 'lo', 'mas', 'me', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'muita', 'muitas', 'muito', 'muitos', 'na', 'nao', 'nas', 'nem', 'nenhum', 'nessa', 'nessas', 'nesta', 'nestas', 'ninguem', 'no', 'nos', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'nunca', 'o', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'pode', 'pude', 'podendo', 'poder', 'poderia', 'poderiam', 'podia', 'podiam', 'pois', 'por', 'porem', 'porque', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeiro', 'primeiros', 'propria', 'proprias', 'proprio', 'proprios', 'quais', 'qual', 'quando', 'quanto', 'quantos', 'que', 'quem', 'sao', 'se', 'seja', 'sejam', 'sem', 'sempre', 'sendo', 'sera', 'serao', 'seu', 'seus', 'si', 'sido', 'so', 'sob', 'sobre', 'sua', 'suas', 'talvez', 'tambem', 'tampouco', 'te', 'tem', 'tendo', 'tenha', 'ter', 'teu', 'teus', 'ti', 'tido', 'tinha', 'tinham', 'toda', 'todas', 'todavia', 'todo', 'todos', 'tu', 'tua', 'tuas', 'tudo', 'ultima', 'ultimas', 'ultimo', 'ultimos', 'um', 'uma', 'umas', 'uns', 'vendo', 'ver', 'vez', 'vindo', 'vir', 'vos']\n",
    "            #análise de frequências\n",
    "            stopWords2 = [\"ca\",\"terra\", \"europa\", \"pros\", \"sr\", \"ninguem\", \"algo\", \"msm\", \"to\", \"vcs\", \"vi\", \"ir\", \"ficar\",\n",
    "                          \"dois\", \"onde\", \"sera\", \"ate\", \"entao\", \"ue\", \"pa\", \"ia\", \"va\", \"qdo\", \"via\", \"qq\",\n",
    "                          \"acho\", \"nada\", \"quero\", \"sim\", \"da\", \"dia\", \"pras\",\n",
    "                          \"aqui\", \"faz\", \"ta\", \"dar\", \"vao\", \"quer\", \"vem\", \"voces\", \"sabe\", \"so\", \"fica\", \"ser\", \"ne\",\n",
    "                          \"ne\", \"estao\", \"sao\", \"voce\", \"nao\", \"tambem\", \"tb\",\n",
    "                          \"volta\", \"ja\", \"rt\", \"vc\", \"ai\", \"la\", \"pro\", \"pra\", \"ve\", \"hj\", \"disso\", \"outro\", \"uns\", \"eh\",\n",
    "                          \"sobre\", \"tao\", \"assim\", \"disse\", \"estar\", \"vou\", \"pode\", \"vez\", \"vai\", \"estar\",\n",
    "                          \"pq\", \"mil\", \"mt\", \"mim\"]\n",
    "            wordlist = tknze(sentence)\n",
    "            stopWords3 = stopwords.words(\"portuguese\")\n",
    "            newWordList = []\n",
    "            for w in wordlist:\n",
    "                if w not in stopWords1 and w not in stopWords2 and w not in stopWords3 and len(w) > 1 and \"kkkk\" not in w:\n",
    "                    newWordList.append(w)\n",
    "\n",
    "            return joinTokens(newWordList)\n",
    "\n",
    "        def removePunctuation(sentence):\n",
    "            wordlist = tknze(sentence)\n",
    "            newWordList = []\n",
    "            for w in wordlist:\n",
    "                newWordList.append(w.translate(str.maketrans(\"\",\"\",string.punctuation)))\n",
    "\n",
    "            return joinTokens(newWordList)\n",
    "\n",
    "        def removeNumbers(sentence):\n",
    "            wordlist = tknze(sentence)\n",
    "            newWordList = []\n",
    "            for w in wordlist:\n",
    "                newWordList.append(w.translate(str.maketrans(\"\",\"\",\"0123456789\")))\n",
    "\n",
    "            return joinTokens(newWordList)\n",
    "\n",
    "        def removeAccents(sentence):\n",
    "            wordlist = tknze(sentence)\n",
    "            newWordList = []\n",
    "            for w in wordlist:\n",
    "                newWord = ''.join(ch for ch in unicodedata.normalize('NFKD', w) if not unicodedata.combining(ch))\n",
    "                newWordList.append(newWord)\n",
    "\n",
    "            return joinTokens(newWordList)\n",
    "\n",
    "        def stemming(sentence):\n",
    "            stemmer = SnowballStemmer(\"portuguese\")\n",
    "            wordlist = tknze(sentence)\n",
    "            newWordList = []\n",
    "            for w in wordlist:\n",
    "                newWordList.append(stemmer.stem(w))\n",
    "\n",
    "            return joinTokens(newWordList)\n",
    "\n",
    "        def lemmatization(sentence):\n",
    "            pass\n",
    "        \n",
    "        #Preprocesing\n",
    "        self.ds_fortuna[\"preprocessing\"] = self.ds_fortuna[\"text\"].apply(tweetTknze)\n",
    "        self.ds_fortuna[\"preprocessing\"] = self.ds_fortuna[\"preprocessing\"].apply(removePunctuation)\n",
    "        self.ds_fortuna[\"preprocessing\"] = self.ds_fortuna[\"preprocessing\"].apply(removeNumbers)\n",
    "        self.ds_fortuna[\"preprocessing\"] = self.ds_fortuna[\"preprocessing\"].apply(removeAccents)\n",
    "        self.ds_fortuna[\"preprocessing\"] = self.ds_fortuna[\"preprocessing\"].apply(removeStopWords)\n",
    "        #self.ds_fortuna[\"preprocessing\"] = self.ds_fortuna[\"preprocessing\"].apply(stemming)\n",
    "        self.ds_fortuna[\"pp_tokens\"] = self.ds_fortuna[\"preprocessing\"].apply(tknze)\n",
    "\n",
    "        self.ds_pelle[\"preprocessing\"] = self.ds_pelle[\"text\"].apply(removePunctuation)\n",
    "        self.ds_pelle[\"preprocessing\"] = self.ds_pelle[\"preprocessing\"].apply(removeNumbers)\n",
    "        self.ds_pelle[\"preprocessing\"] = self.ds_pelle[\"preprocessing\"].apply(removeAccents)\n",
    "        self.ds_pelle[\"preprocessing\"] = self.ds_pelle[\"preprocessing\"].apply(removeStopWords)\n",
    "        #self.ds_pelle[\"preprocessing\"] = self.ds_pelle[\"preprocessing\"].apply(stemming)\n",
    "        self.ds_pelle[\"pp_tokens\"] = self.ds_pelle[\"preprocessing\"].apply(tknze)\n",
    "        \n",
    "        #Functions\n",
    "        def averageWordVectors(words, model, vocabulary, num_features):\n",
    "            feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "            nwords = 0.\n",
    "\n",
    "            for word in words:\n",
    "                if word in vocabulary: \n",
    "                    nwords = nwords + 1.\n",
    "                    feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "            if nwords:\n",
    "                feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "            return feature_vector\n",
    "\n",
    "\n",
    "        def averagedWordVectorizer(corpus, model, num_features):\n",
    "            vocabulary = set(model.index2word)\n",
    "            features = [averageWordVectors(tokenized_sentence, model, vocabulary,\n",
    "                                           num_features) for tokenized_sentence in corpus]\n",
    "            return np.array(features)\n",
    "\n",
    "        def removeAccentsW2V(model):\n",
    "            l = list(model.vocab)\n",
    "            for w in l:\n",
    "                model.vocab[''.join(ch for ch in unicodedata.normalize(\"NFKD\",\n",
    "                                    w) if not unicodedata.combining(ch))] = model.vocab.pop(w)\n",
    "            return model\n",
    "        \n",
    "        self.w2v_model = KeyedVectors.load_word2vec_format(\"datasets/cbow_s100_USP.txt\")\n",
    "        self.w2v_model = removeAccentsW2V(self.w2v_model) #Removing accents\n",
    "        \n",
    "        self.X_fortuna_w2v = averagedWordVectorizer(corpus=self.ds_fortuna[\"pp_tokens\"].tolist(), model=self.w2v_model,\n",
    "                                                    num_features=100) #Dataset Fortuna Embeddings\n",
    "        self.X_offPelle_w2v = averagedWordVectorizer(corpus=self.ds_pelle[self.ds_pelle[\"label\"] == 1][\"pp_tokens\"].tolist(), \n",
    "                                                     model=self.w2v_model, num_features=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fortunaWordCloud(self, col, label):\n",
    "        dataFrame = self.ds_fortuna\n",
    "               \n",
    "        df_filtrado = dataFrame.query(\"label == '{}'\".format(label))\n",
    "        allWords = ' '.join([s for s in df_filtrado[col]])\n",
    "\n",
    "        cloud = WordCloud(width = 900, height = 600,\n",
    "                                  max_font_size = 120,\n",
    "                                  collocations = False).generate(allWords)\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.imshow(cloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    def pelleWordCloud(self, col, label):\n",
    "        dataFrame = self.ds_pelle\n",
    "        \n",
    "        df_filtrado = dataFrame.query(\"label == '{}'\".format(label))\n",
    "        allWords = ' '.join([s for s in df_filtrado[col]])\n",
    "\n",
    "        cloud = WordCloud(width = 900, height = 600,\n",
    "                                  max_font_size = 120,\n",
    "                                  collocations = False).generate(allWords)\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.imshow(cloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    \n",
    "    def fortunaFrequency(self, col, label):\n",
    "             \n",
    "        dataFrame = self.ds_fortuna\n",
    "               \n",
    "        df_filtrado = dataFrame.query(\"label == '{}'\".format(label))\n",
    "        allWords = ' '.join([s for s in df_filtrado[col]])\n",
    "\n",
    "        spaceTknzr = tokenize.WhitespaceTokenizer()\n",
    "        sentences = spaceTknzr.tokenize(allWords)\n",
    "        frequency = FreqDist(sentences)\n",
    "\n",
    "        df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()), \"Frequency\": list(frequency.values())})\n",
    "        return df_frequency.nlargest(columns = \"Frequency\", n = 50)\n",
    "    \n",
    "    def pellefrequency(self, col, label):\n",
    "        \n",
    "        dataFrame = self.ds_pelle\n",
    "        \n",
    "        df_filtrado = dataFrame.query(\"label == '{}'\".format(label))\n",
    "        allWords = ' '.join([s for s in df_filtrado[col]])\n",
    "\n",
    "        spaceTknzr = tokenize.WhitespaceTokenizer()\n",
    "        sentences = spaceTknzr.tokenize(allWords)\n",
    "        frequency = FreqDist(sentences)\n",
    "\n",
    "        df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()), \"Frequency\": list(frequency.values())})\n",
    "        return df_frequency.nlargest(columns = \"Frequency\", n = 50)\n",
    "    \n",
    "    def fortunaPareto(self, col, n, df=\"fortuna\"):\n",
    "        \n",
    "        dataFrame = self.ds_fortuna\n",
    "        \n",
    "        spaceTknzr = tokenize.WhitespaceTokenizer()\n",
    "        allWords = \" \".join([s for s in dataFrame[col]])\n",
    "        sentences = spaceTknzr.tokenize(allWords)\n",
    "        frequency = FreqDist(sentences)\n",
    "        df_frequency = pd.DataFrame({\"Palavra\": list(frequency.keys()),\n",
    "                                     \"Frequência\": list(frequency.values())})\n",
    "        df_frequency = df_frequency.nlargest(columns = \"Frequência\", n = n)\n",
    "        plt.figure(figsize=(14,12))\n",
    "        ax = sns.barplot(data = df_frequency, x = \"Palavra\", y = \"Frequência\")\n",
    "        ax.set(ylabel = \"Contagem\")\n",
    "        plt.show()\n",
    "        \n",
    "    def pellePareto(self, col, n, df=\"fortuna\"):\n",
    "        \n",
    "        dataFrame = self.ds_pelle\n",
    "        \n",
    "        spaceTknzr = tokenize.WhitespaceTokenizer()\n",
    "        allWords = \" \".join([s for s in dataFrame[col]])\n",
    "        sentences = spaceTknzr.tokenize(allWords)\n",
    "        frequency = FreqDist(sentences)\n",
    "        df_frequency = pd.DataFrame({\"Palavra\": list(frequency.keys()),\n",
    "                                     \"Frequência\": list(frequency.values())})\n",
    "        df_frequency = df_frequency.nlargest(columns = \"Frequência\", n = n)\n",
    "        plt.figure(figsize=(14,12))\n",
    "        ax = sns.barplot(data = df_frequency, x = \"Palavra\", y = \"Frequência\")\n",
    "        ax.set(ylabel = \"Contagem\")\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusionMatrix(y_true, y_pred, img_name, cmap=plt.cm.Blues):\n",
    "        \n",
    "        classes=['0','1']\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        # Only use the labels that appear in the data\n",
    "        classes = list(['no','yes'])#classes[unique_labels(y_true, y_pred)]\n",
    "        cm_n = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title=img_name,\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        fmt_n = '.2f'\n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], fmt)+' ('+ format(cm_n[i, j], fmt_n) +')',\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        #fig.tight_layout()\n",
    "        \n",
    "        return cm, fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def rf_random_search(X_train, y_train, n_iter, random_state):\n",
    "        \n",
    "        rf_model = RandomForestClassifier(random_state=random_state)\n",
    "        \n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = [\"auto\", \"sqrt\"]\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Class weight\n",
    "        class_weight = [\"balanced\", {1: 1}, {1: 2}, {1: 4}, {1: 6}, {1: 8}]\n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap,\n",
    "                       'class_weight': class_weight}\n",
    "        \n",
    "        rf_random = RandomizedSearchCV(estimator=rf_model, param_distributions=random_grid,\n",
    "                                       n_iter=n_iter, cv=2, verbose=2, random_state=random_state,\n",
    "                                       n_jobs=-1, scoring='f1')\n",
    "        \n",
    "        rf_random.fit(X_train, y_train)\n",
    "        \n",
    "        return rf_random\n",
    "    \n",
    "    def experiment(self, feature, add_Pelle=False, rf_rs_n_iter = 100,\n",
    "                   ngram_range=(1,1), splitting=\"STRAT\", rs = 42):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        #Data split\n",
    "        ss_split = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=rs)\n",
    "        s_split = ShuffleSplit(n_splits=10, test_size=0.2, random_state=rs)\n",
    "        \n",
    "        if splitting == \"STRAT\":\n",
    "        \n",
    "            splitter = ss_split #Stratified\n",
    "        \n",
    "        elif splitting == \"SHUFFLE\":\n",
    "            \n",
    "            splitter = s_split #Shuffle\n",
    "        \n",
    "        #Classifiers\n",
    "        clf_lr = LogisticRegression(solver = \"lbfgs\", class_weight=\"balanced\")\n",
    "        clf_svm = svm.SVC(kernel=\"linear\", class_weight=\"balanced\")\n",
    "        clf_rf = RandomForestClassifier(random_state=42)\n",
    "        clf_nb = GaussianNB()\n",
    "        \n",
    "        #Features       \n",
    "        if feature == \"BOW\" or feature == \"TFIDF\":\n",
    "            \n",
    "            bow_vectorizer = CountVectorizer(lowercase=False, ngram_range=ngram_range, analyzer=\"word\")\n",
    "            tfidf_vectorizer = TfidfVectorizer(lowercase=False, ngram_range=ngram_range, analyzer=\"word\")\n",
    "        \n",
    "            X = self.ds_fortuna[\"preprocessing\"] #features\n",
    "            y = self.ds_fortuna[\"label\"] #target\n",
    "        \n",
    "        elif feature == \"W2V\":\n",
    "                                   \n",
    "            X = self.X_fortuna_w2v #features\n",
    "            y = np.array(self.ds_fortuna[\"label\"].tolist()) #target\n",
    "        \n",
    "        clfs = [clf_lr, clf_svm, clf_rf, clf_nb]\n",
    "        \n",
    "        exp_id = f\"EXP{self.exp_counter}\"\n",
    "        exp_outcomes = {}\n",
    "        exp_outcomes[\"id\"] = exp_id\n",
    "        rs_outcomes = {}\n",
    "        \n",
    "        #experiment logs\n",
    "        tags = f\"[{feature}]\"\n",
    "        if feature != \"W2V\": tags += f\"[NG{ngram_range}]\"\n",
    "        if add_Pelle: tags += \"[PELLE]\"\n",
    "        \n",
    "        exp_outcomes[\"tags\"] = tags\n",
    "        exp_outcomes[\"feature\"] = f\"{feature}\"\n",
    "        exp_outcomes[\"ngram\"] = f\"{ngram_range}\"\n",
    "        if add_Pelle:\n",
    "            exp_outcomes[\"pelle\"] = 1\n",
    "        else: \n",
    "            exp_outcomes[\"pelle\"] = 0\n",
    "        exp_outcomes[\"created\"] = f\"{date.today()}\"\n",
    "        \n",
    "        rs_outcomes[\"tags\"] = tags\n",
    "        rs_outcomes[\"feature\"] = f\"{feature}\"\n",
    "        rs_outcomes[\"ngram\"] = f\"{ngram_range}\"\n",
    "        if add_Pelle:\n",
    "            rs_outcomes[\"pelle\"] = 1\n",
    "        else: \n",
    "            rs_outcomes[\"pelle\"] = 0\n",
    "        exp_outcomes[\"created\"] = f\"{date.today()}\"\n",
    "        rs_outcomes[\"created\"] = f\"{date.today()}\"\n",
    "        \n",
    "        split_counter = 0\n",
    "        \n",
    "        rs_ok = False\n",
    "        \n",
    "        for train_index, test_index in splitter.split(X, y.tolist()):\n",
    "            \n",
    "            split_counter += 1\n",
    "            \n",
    "            exp_outcomes[\"split\"] = split_counter\n",
    "            rs_outcomes[\"split\"] = split_counter\n",
    "            \n",
    "            for clf in clfs:\n",
    "                \n",
    "                X_train, X_test = X[train_index], X[test_index] \n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "                if add_Pelle:\n",
    "                    #Adding Off_Pelle in the train split\n",
    "                    off_pelle = self.ds_pelle[self.ds_pelle[\"label\"] == 1]\n",
    "                    #Verifying feature type\n",
    "                    X_train = np.append(X_train, self.X_offPelle_w2v, axis=0) if feature == \"W2V\" else X_train.append(off_pelle[\"preprocessing\"], ignore_index=True)\n",
    "                    y_train = np.append(y_train, np.array(off_pelle[\"label\"].tolist()), axis=0) if feature == \"W2V\" else y_train.append(off_pelle[\"label\"], ignore_index=True)\n",
    "                \n",
    "                clf_nb_name = f\"{type(clf_nb).__name__}\"\n",
    "                clf_rf_name = f\"{type(clf_rf).__name__}\"\n",
    "                clf_lr_name = f\"{type(clf_lr).__name__}\"\n",
    "                clf_svm_name = f\"{type(clf_svm).__name__}\"\n",
    "                clf_name = f\"{type(clf).__name__}\"\n",
    "                \n",
    "                #adapting classifiers inputs\n",
    "                if feature == \"BOW\":\n",
    "                        \n",
    "                    X_train = bow_vectorizer.fit_transform(X_train).toarray() if clf_name == clf_nb_name else bow_vectorizer.fit_transform(X_train)\n",
    "                    X_test = bow_vectorizer.transform(X_test).toarray() if clf_name == clf_nb_name else bow_vectorizer.transform(X_test)\n",
    "                \n",
    "                elif feature == \"TFIDF\":\n",
    "                    \n",
    "                    X_train = tfidf_vectorizer.fit_transform(X_train).toarray() if clf_name == clf_nb_name else tfidf_vectorizer.fit_transform(X_train)\n",
    "                    X_test = tfidf_vectorizer.transform(X_test).toarray() if clf_name == clf_nb_name else tfidf_vectorizer.transform(X_test)\n",
    "                \n",
    "                #Random Forest - Random Search\n",
    "                if clf_name == clf_rf_name and not rs_ok:\n",
    "                    rf_random = Model.rf_random_search(X_train, y_train, rf_rs_n_iter, rs)      \n",
    "                    \n",
    "                    #Getting the best RS parameters\n",
    "                    clf = rf_random.best_estimator_\n",
    "                    clfs[2] = rf_random.best_estimator_\n",
    "                    \n",
    "                    #Random Search Experiments\n",
    "                    for i in range(rf_rs_n_iter):\n",
    "                        rs_outcomes[\"n_sample\"] = i\n",
    "                        rs_outcomes[\"n_estimators\"] = rf_random.cv_results_[\"params\"][i][\"n_estimators\"]\n",
    "                        rs_outcomes[\"max_features\"] = rf_random.cv_results_[\"params\"][i][\"max_features\"]\n",
    "                        rs_outcomes[\"max_depth\"] = rf_random.cv_results_[\"params\"][i][\"max_depth\"]\n",
    "                        rs_outcomes[\"min_samples_split\"] = rf_random.cv_results_[\"params\"][i][\"min_samples_split\"]\n",
    "                        rs_outcomes[\"min_samples_leaf\"] = rf_random.cv_results_[\"params\"][i][\"min_samples_leaf\"]\n",
    "                        rs_outcomes[\"bootstrap\"] = rf_random.cv_results_[\"params\"][i][\"bootstrap\"]\n",
    "                        rs_outcomes[\"class_weight\"] = rf_random.cv_results_[\"params\"][i][\"class_weight\"]\n",
    "                        rs_outcomes[\"f1_mean_test_score\"] = round(rf_random.cv_results_[\"mean_test_score\"][i],2)\n",
    "                        self.rs_outcomes = self.rs_outcomes.append(rs_outcomes, ignore_index=True)\n",
    "                    \n",
    "                    rs_ok = True\n",
    "                        \n",
    "                #training\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                #predicting\n",
    "                pred = clf.predict(X_test)\n",
    "                \n",
    "                #metrics\n",
    "                ac = accuracy_score(y_test, pred)\n",
    "                f1 = f1_score(y_test, pred)\n",
    "                roc_auc = roc_auc_score(y_test, pred)\n",
    "                rec = recall_score(y_test, pred)\n",
    "                pr = precision_score(y_test, pred)\n",
    "                \n",
    "                #experiment logs\n",
    "                if clf_name == clf_lr_name:\n",
    "                    exp_outcomes[\"acc_lr\"] = round(ac,2)\n",
    "                    exp_outcomes[\"rec_lr\"] = round(rec,2)\n",
    "                    exp_outcomes[\"pr_lr\"] = round(pr,2)\n",
    "                    exp_outcomes[\"f1_lr\"] = round(f1,2)\n",
    "                    exp_outcomes[\"roc_auc_lr\"] = round(roc_auc,2)\n",
    "                elif clf_name == clf_nb_name:\n",
    "                    exp_outcomes[\"acc_nb\"] = round(ac,2)\n",
    "                    exp_outcomes[\"rec_nb\"] = round(rec,2)\n",
    "                    exp_outcomes[\"pr_nb\"] = round(pr,2)\n",
    "                    exp_outcomes[\"f1_nb\"] = round(f1,2)\n",
    "                    exp_outcomes[\"roc_auc_nb\"] = round(roc_auc,2)\n",
    "                elif clf_name == clf_svm_name:\n",
    "                    exp_outcomes[\"acc_svm\"] = round(ac,2)\n",
    "                    exp_outcomes[\"rec_svm\"] = round(rec,2)\n",
    "                    exp_outcomes[\"pr_svm\"] = round(pr,2)\n",
    "                    exp_outcomes[\"f1_svm\"] = round(f1,2)\n",
    "                    exp_outcomes[\"roc_auc_svm\"] = round(roc_auc,2)\n",
    "                elif clf_name == clf_rf_name:\n",
    "                    exp_outcomes[\"acc_rf\"] = round(ac,2)\n",
    "                    exp_outcomes[\"rec_rf\"] = round(rec,2)\n",
    "                    exp_outcomes[\"pr_rf\"] = round(pr,2)\n",
    "                    exp_outcomes[\"f1_rf\"] = round(f1,2)\n",
    "                    exp_outcomes[\"roc_auc_rf\"] = round(roc_auc,2)\n",
    "                \n",
    "                #Confusion matrix\n",
    "                \n",
    "                img_name = f\"{feature}_{clf_name}_split({split_counter})\"\n",
    "                if feature != \"W2V\": img_name += f\"_NG{ngram_range[1]}\"\n",
    "                if add_Pelle: img_name += f\"_PELLE\"\n",
    "                \n",
    "                cm, fig = Model.confusionMatrix(y_test, pred, \n",
    "                                                img_name=img_name)\n",
    "                \n",
    "                plt.savefig(f\"experiments/img/{img_name}.png\")\n",
    "                plt.close('all')\n",
    "                \n",
    "                #TN = cm[0][0]\n",
    "                #FN = cm[1][0]\n",
    "                #TP = cm[1][1]\n",
    "                #FP = cm[0][1]\n",
    "        \n",
    "            exp_outcomes[\"elapsed\"] = time.time() - start_time\n",
    "            self.exp_counter += 1\n",
    "            if feature == \"BOW\":\n",
    "                self.bow_outcomes = self.bow_outcomes.append(exp_outcomes, ignore_index=True)\n",
    "            elif feature == \"TFIDF\":\n",
    "                self.tfidf_outcomes = self.tfidf_outcomes.append(exp_outcomes, ignore_index=True)\n",
    "            elif feature == \"W2V\":\n",
    "                self.w2v_outcomes = self.w2v_outcomes.append(exp_outcomes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model object\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 51.8min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 56.4min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - NGRAM(1,1); Pelle(0)\n",
    "model.experiment(\"BOW\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,1), splitting=\"STRAT\", rs=42)\n",
    "\n",
    "model.experiment(\"TFIDF\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,1), splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 78.3min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 84.4min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - NGRAM(1,2); Pelle(0)\n",
    "model.experiment(\"BOW\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,2), splitting=\"STRAT\", rs=42)\n",
    "\n",
    "model.experiment(\"TFIDF\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,2), splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 90.6min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 96.9min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - NGRAM(1,3); Pelle(0)\n",
    "model.experiment(\"BOW\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,3), splitting=\"STRAT\", rs=42)\n",
    "\n",
    "model.experiment(\"TFIDF\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,3), splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 52.1min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 58.6min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - W2V; Pelle(0)\n",
    "model.experiment(\"W2V\", add_Pelle=False, rf_rs_n_iter=200,\n",
    "                 splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 23.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 59.0min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 63.7min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - NGRAM(1,1); Pelle(1)\n",
    "model.experiment(\"BOW\", add_Pelle=True,rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,1), splitting=\"STRAT\", rs=42)\n",
    "\n",
    "model.experiment(\"TFIDF\", add_Pelle=True, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,1), splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 32.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 91.9min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 101.3min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - NGRAM(1,2); Pelle(1)\n",
    "model.experiment(\"BOW\", add_Pelle=True, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,2), splitting=\"STRAT\", rs=42)\n",
    "\n",
    "model.experiment(\"TFIDF\", add_Pelle=True, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,2), splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 55.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 158.2min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 169.6min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - NGRAM(1,3); Pelle(1)\n",
    "model.experiment(\"BOW\", add_Pelle=True, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,3), splitting=\"STRAT\", rs=42)\n",
    "\n",
    "model.experiment(\"TFIDF\", add_Pelle=True, rf_rs_n_iter=200,\n",
    "                 ngram_range=(1,3), splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 57.9min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 65.3min finished\n"
     ]
    }
   ],
   "source": [
    "#Experimento - W2V; Pelle(1)\n",
    "model.experiment(\"W2V\", add_Pelle=True, rf_rs_n_iter=200,\n",
    "                 splitting=\"STRAT\", rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando arquivos\n",
    "model.bow_outcomes.to_csv(\"experiments/BOW_Experiments.csv\", index=False, encoding=\"utf-8\")\n",
    "model.tfidf_outcomes.to_csv(\"experiments/TFIDF_Experiments.csv\", index=False, encoding=\"utf-8\")\n",
    "model.w2v_outcomes.to_csv(\"experiments/W2V_Experiments.csv\", index=False, encoding=\"utf-8\")\n",
    "model.rs_outcomes.to_csv(\"experiments/RS_Experiments.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
